{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6YeoOEpJpy4"
   },
   "source": [
    "# **Biomarker regions segmentation with 3D U-net**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LqQkmVuLi6o"
   },
   "source": [
    "#### 1. Introduction\n",
    "\n",
    "In this section we will segment gray matter and subcortical nuclei from preprocessed MRI image.\n",
    "\n",
    "*Proceeding with this Notebook you confirm your personal acess [to the data](https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release). \n",
    " And your agreement on data [terms and conditions](https://www.humanconnectome.org/study/hcp-young-adult/data-use-terms).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MSzOne1OItfJ"
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "qZVui824s_Mr",
    "outputId": "71063895-43b9-4e69-b85e-7778192f068a"
   },
   "outputs": [],
   "source": [
    "# !pip install --quiet --upgrade comet_ml\n",
    "from comet_ml import Experiment\n",
    "    \n",
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key='SKty3eCyCLDyXicElR2IoeZpi',\n",
    "    project_name='segment-brain'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ii8ogiHEONTi"
   },
   "source": [
    "### Check the experiments:\n",
    "\n",
    "**Baseline** - \"6 classes, 4 encoding blocks, 8 out, Patch based, 64 batch, crop\"\n",
    "https://www.comet.ml/kondratevakate/mri-segmentation-2021/view/Q6KVHNSRxQ22hC0oM7NL1rF1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evtOoVX_PDvX"
   },
   "source": [
    "#### 2. Mounting Google drive\n",
    "\n",
    "Mounting Google Drive to Collab Notebook. You should go with the link and enter your personal authorization code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "BT1UYpwaItfS",
    "outputId": "60fdfb4c-6d21-4bd4-ee53-8d0acd6098bc"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zLyMt9CPpOI"
   },
   "source": [
    "data_dir = '/content/drive/My Drive/Skoltech Neuroimaging/NeuroML2020/data/seminars/anat/fs_segmentation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "abYD1633PqJF",
    "outputId": "6069f6ea-4851-4e08-e0ff-1dda76e29938"
   },
   "source": [
    "data_list = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6xjOyP3Qs8X"
   },
   "source": [
    "#### 3. Defining the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fUl_R5bgRyav"
   },
   "source": [
    "Defining the working dataset, there:\n",
    "\n",
    " 1. `norm` - normalised `T1` image processed with Freesurfer 6.0,\n",
    "\n",
    " 2. `aparc+aseg` segmentation mask for gray matter and subcortical nuclei from Freesufer 6.0 `recon all` pipeline.\n",
    "\n",
    " And U-net model will treat `norm` image as input and `aparc+aseg` as target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/neuro-ml-2002/anat/fs_segmentation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TU3ZwxcrRx6-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels_dir = '/home/neuro-ml-2002/anat/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6eBoVB9ZMu4b"
   },
   "source": [
    "Defining new `pd.Dataframe()` with `Subject`, `norm` and `target` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OfWcsdESMolg"
   },
   "outputs": [],
   "source": [
    "data_list = pd.DataFrame(columns = ['Subject', 'norm', 'aseg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./anat/unrestricted_hcp_freesurfer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMI78_wsL1Ff"
   },
   "outputs": [],
   "source": [
    "data_list['Subject'] = labels['Subject']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y78CEixkXxf0"
   },
   "source": [
    "Iterating through files and `Subjects` in ID list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "x06eCtN9Xcet",
    "outputId": "fcef2c2f-dbb3-404d-a1eb-bdade87c5ec7"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "for i in tqdm(os.listdir(data_dir)):\n",
    "    for j in range(0, len(data_list['Subject'])):\n",
    "\n",
    "        if str(data_list['Subject'].iloc[j]) in i:\n",
    "            if 'norm' in i: # copydaing path to the column norm\n",
    "                data_list['norm'].iloc[j] = data_dir +'/'+ i\n",
    "            elif 'aseg' in i: # copying path to second column\n",
    "                data_list['aseg'].iloc[j] = data_dir +'/'+ i\n",
    "\n",
    "data_list.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "eey6753TOthV",
    "outputId": "58f6852c-9557-48c4-88e0-946062c1f1b9"
   },
   "outputs": [],
   "source": [
    "data_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKEpEH31Q9YD"
   },
   "source": [
    "Let's have a closer look on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "ORhOxckhQ8T4",
    "outputId": "95738729-f100-427e-b132-de87f614e4ff"
   },
   "outputs": [],
   "source": [
    "# !pip install --quiet --upgrade nilearn\n",
    "import nilearn\n",
    "from nilearn import plotting\n",
    "\n",
    "# visualising normalised image\n",
    "img = nilearn.image.load_img(data_list['norm'].iloc[0])\n",
    "plotting.plot_anat(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "Ja5weGNzRnkx",
    "outputId": "69ac300a-f550-45ce-e1c1-c64795ee51ea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualising segmentation\n",
    "img = nilearn.image.load_img(data_list['aseg'].iloc[0])\n",
    "plotting.plot_anat(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "pU0kWyDMR6w4",
    "outputId": "ce97c238-3ee8-4360-d9f3-982d99c03aee"
   },
   "outputs": [],
   "source": [
    "np.unique(np.asanyarray(img.dataobj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subjects = [100206, 100307, 100408]\n",
    "test_norm_dir = './test'\n",
    "\n",
    "testing_data_list = pd.DataFrame({\n",
    "    'Subject': test_subjects,\n",
    "    'norm': [f'{test_norm_dir}/HCP_T1_fs6_{subject}_norm.nii.gz' for subject in test_subjects],\n",
    "    'aseg': [f'{test_norm_dir}/HCP_T1_fs6_{subject}_aparc+aseg.nii.gz' for subject in test_subjects]\n",
    "})\n",
    "\n",
    "\n",
    "testing_data_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUZAi8kCQQA_"
   },
   "source": [
    "#### 4. Writing dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrYEX6rRRosY"
   },
   "source": [
    "We will use `TorchIO` library: https://torchio.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1vauzBnRMhA"
   },
   "outputs": [],
   "source": [
    "# !pip install --quiet --upgrade torchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "hY1xztsiQPPO",
    "outputId": "e35ad492-d8f3-4183-9702-fcf5424764fc"
   },
   "outputs": [],
   "source": [
    "import torchio \n",
    "import enum\n",
    "\"\"\"\n",
    "    Code adapted from: https://github.com/fepegar/torchio#credits\n",
    "\n",
    "        Credit: Pérez-García et al., 2020, TorchIO: \n",
    "        a Python library for efficient loading, preprocessing, \n",
    "        augmentation and patch-based sampling of medical images in deep learning.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "MRI = 'MRI'\n",
    "LABEL = 'LABEL'\n",
    "\n",
    "class Action(enum.Enum):\n",
    "    TRAIN = 'Training'\n",
    "    VALIDATE = 'Validation'\n",
    "\n",
    "def get_torchio_dataset(inputs, targets, transform):\n",
    "    \"\"\"\n",
    "    The function creates dataset from the list of files from cunstumised dataloader.\n",
    "    \"\"\"\n",
    "    subjects = []\n",
    "    for (image_path, label_path) in zip(inputs, targets):\n",
    "        subject_dict = {\n",
    "            MRI : torchio.Image(image_path, torchio.INTENSITY),\n",
    "            LABEL: torchio.Image(label_path, torchio.LABEL),\n",
    "        }\n",
    "        subject = torchio.Subject(subject_dict)\n",
    "        subjects.append(subject)\n",
    "    \n",
    "    if transform:\n",
    "        dataset = torchio.SubjectsDataset(subjects, transform = transform)\n",
    "    elif not transform:\n",
    "        dataset = torchio.SubjectsDataset(subjects)\n",
    "    \n",
    "    return dataset, subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yCUWSvOHkASt"
   },
   "outputs": [],
   "source": [
    "data, subjects = get_torchio_dataset(data_list['norm'], data_list['aseg'], False)\n",
    "testing_data, testing_subjects = get_torchio_dataset(testing_data_list['norm'], testing_data_list['aseg'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "N_soFhcYkijr",
    "outputId": "9b5c5284-dbfe-4a0c-96ed-c025378ecfbd"
   },
   "outputs": [],
   "source": [
    "data[0]['MRI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_JR8HbiPCGl"
   },
   "source": [
    "#### 3. Writing visualization tools for torch tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f26Pc35TPAaO"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel\n",
    "\n",
    "def plot_central_cuts(img, title=\"\"):\n",
    "    \"\"\"\n",
    "    param image: tensor or np array of shape (CxDxHxW) if t is None\n",
    "    \"\"\"\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.numpy()\n",
    "        if (len(img.shape) > 3):\n",
    "            img = img[0,:,:,:]\n",
    "                \n",
    "    elif isinstance(img, nibabel.nifti1.Nifti1Image):    \n",
    "        img = img.get_fdata()\n",
    "   \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(3 * 6, 6))\n",
    "    axes[0].imshow(img[ img.shape[0] // 2, :, :])\n",
    "    axes[1].imshow(img[ :, img.shape[1] // 2, :])\n",
    "    axes[2].imshow(img[ :, :, img.shape[2] // 2])\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_predicted(img, seg, gt, delta = 0, title=\"\"):\n",
    "    \"\"\"\n",
    "    param image: tensor or np array of shape (CxDxHxW) if t is None\n",
    "    \"\"\"\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.cpu().numpy()\n",
    "        if (len(img.shape) == 5):\n",
    "            img = img[0,0,:,:,:]\n",
    "        elif (len(img.shape) == 4):\n",
    "            img = img[0,:,:,:]\n",
    "                \n",
    "    elif isinstance(img, nibabel.nifti1.Nifti1Image):    \n",
    "        img = img.get_fdata()\n",
    "        \n",
    "    if isinstance(seg, torch.Tensor):\n",
    "        seg= seg[0].cpu().numpy()#.astype(np.uint8)\n",
    "            \n",
    "    ### MY ADDITION: STARTS HERE\n",
    "    if isinstance(gt, torch.Tensor):\n",
    "        gt= gt[0].cpu().numpy()#.astype(np.uint8)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 20))\n",
    "    i = img.shape[0] // 2 + delta\n",
    "    j = seg.shape[0] // 2 + delta\n",
    "    \n",
    "#     overlay is less useful than gt for comet logging\n",
    "#     intersect = seg[ i, ...]*100 + img[ j, ...]\n",
    "    axes[0, 0].imshow(img[ i, ...])\n",
    "    axes[0, 0].set_title('input', fontsize=16)\n",
    "    axes[0, 1].imshow(gt[ j, ...])\n",
    "    axes[0, 1].set_title('gt', fontsize=16)\n",
    "#     axes[0, 2].imshow(intersect, label='overlay')\n",
    "    axes[0, 2].imshow(seg[ j, ...])\n",
    "    axes[0, 2].set_title('predicted segmentation', fontsize=16)\n",
    "    \n",
    "#     intersect = seg[ :, i, :]*100 + img[ :, j, :]\n",
    "    axes[1, 0].imshow(img[ :, i, :])\n",
    "    axes[1, 1].imshow(gt[ :, j, :])\n",
    "#     axes[1, 2].imshow(intersect)\n",
    "    axes[1, 2].imshow(seg[ :, j, :])\n",
    "    \n",
    "#     intersect = seg[ ..., j]*100 + img[ ..., i]\n",
    "    axes[2, 0].imshow(img[ ..., i])\n",
    "    axes[2, 1].imshow(gt[ ..., j])\n",
    "#     axes[2, 2].imshow(intersect)\n",
    "    axes[2, 2].imshow(seg[ ..., j])\n",
    "    \n",
    "    for ax in axes.ravel():\n",
    "        ax.axis('off')\n",
    "    fig.tight_layout();\n",
    "    \n",
    "    return fig # return figure for comet_ml\n",
    "    ### MY ADDITION: ENDS HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9YsFfH8QNXm"
   },
   "source": [
    "The class `dataset` inherits from `torch.utils.data.Dataset.` It receives as input a list of `torchio.Subject` instances and an optional `torchio.transforms.Transform.`\n",
    "\n",
    "The inputs to the subject class are instances of torchio.Image, such as torchio.ScalarImage or torchio.LabelMap. The image class will be used by the transforms to decide whether or not to perform the operation. For example, spatial transforms must apply to both, but intensity transforms must apply to scalar images only.\n",
    "\n",
    "https://torchio.readthedocs.io/data/dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LCQr-LiJrRML"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchio import AFFINE, DATA, PATH, TYPE, STEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "id": "b5Cr6UL5ItfU",
    "outputId": "10f27f30-2938-4aa3-fcc2-d9e23877e8a6"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset size: {}\".format(len(data)))\n",
    "img = data[0][MRI]\n",
    "seg = data[0][LABEL]\n",
    "print(\"Image shape: {}\".format(img.shape))\n",
    "print(\"Segmentation shape: {}\".format(seg.shape))\n",
    "plot_central_cuts(img[DATA])\n",
    "plot_central_cuts(seg[DATA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size: {}\".format(len(data)))\n",
    "img = data[210][MRI]\n",
    "seg = data[210][LABEL]\n",
    "print(\"Image shape: {}\".format(img.shape))\n",
    "print(\"Segmentation shape: {}\".format(seg.shape))\n",
    "plot_central_cuts(img[DATA])\n",
    "plot_central_cuts(seg[DATA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['MRI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data[0]['MRI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose cropping based on non-zero values of MRI image - maybe all images have padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop(subjects):\n",
    "    crop = {i: (256, 0) for i in range(3)}\n",
    "    \n",
    "    for subj in tqdm(subjects):\n",
    "        subj_bool = subj['MRI']['data'][0] != 0\n",
    "        \n",
    "        ax_zero_cut = subj_bool.max(dim=2).values.max(dim=1).values.data.numpy()\n",
    "        ax_one_cut = subj_bool.max(dim=2).values.max(dim=0).values.data.numpy()\n",
    "        ax_two_cut = subj_bool.max(dim=1).values.max(dim=0).values.data.numpy()\n",
    "        \n",
    "        ax_zero_min, ax_zero_max = np.where(ax_zero_cut)[0][[0, -1]]\n",
    "        ax_one_min, ax_one_max = np.where(ax_one_cut)[0][[0, -1]]\n",
    "        ax_two_min, ax_two_max = np.where(ax_two_cut)[0][[0, -1]]\n",
    "        \n",
    "        crop[0] = (min(crop[0][0], ax_zero_min), max(crop[0][1], ax_zero_max + 1))\n",
    "        crop[1] = (min(crop[1][0], ax_one_min), max(crop[1][1], ax_one_max + 1))\n",
    "        crop[2] = (min(crop[2][0], ax_two_min), max(crop[2][1], ax_two_max + 1))\n",
    "    \n",
    "    for i in range(3):\n",
    "        crop[i] = (crop[i][0], 256 - crop[i][1])\n",
    "    \n",
    "    \n",
    "    return (crop[1][0], crop[1][1], crop[0][0], crop[0][1], crop[2][0], crop[2][1])\n",
    "\n",
    "# crop = get_crop(subjects)\n",
    "crop = (49, 22, 49, 47, 19, 28)\n",
    "print(crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGRajfc3Itfc"
   },
   "source": [
    "## 2. Whole brain segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irNwaqb6_q-c"
   },
   "source": [
    "Let's define the experiment for whole brain segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-xODpTdPbvK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models.vgg import vgg11_bn\n",
    "from torch.autograd import Function, Variable\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from optparse import OptionParser\n",
    "import time\n",
    "\n",
    "import torchio\n",
    "from torchio import transforms\n",
    "\n",
    "# !pip install --quiet --upgrade unet \n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from unet import UNet\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, ShuffleSplit\n",
    "\n",
    "import warnings\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subjects = len(data)\n",
    "\n",
    "training_split_ratio = 0.9\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # several_transforms,\n",
    "    transforms.Crop(crop),\n",
    "    transforms.Pad(4)\n",
    "])\n",
    "\n",
    "validation_transform = None\n",
    "\n",
    "training_subjects, validation_subjects = train_test_split(\n",
    "    subjects, train_size=training_split_ratio, shuffle=True, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "x524TiSBt7KK",
    "outputId": "e3f87ce8-24fd-4ebb-869e-15628cdc922f"
   },
   "outputs": [],
   "source": [
    "# training_subjects = subjects[:20]\n",
    "# validation_subjects = subjects[20:40] # experimenting just on 20 first subjects\n",
    "def get_sets(train_subjects, val_subjects, test_subjects, train_transform=None, val_transform=None):\n",
    "    training_set = torchio.SubjectsDataset(\n",
    "        train_subjects, transform=train_transform)\n",
    "\n",
    "    validation_set = torchio.SubjectsDataset(\n",
    "        val_subjects, transform=val_transform)\n",
    "\n",
    "    testing_set = torchio.SubjectsDataset(\n",
    "        test_subjects, transform=val_transform)\n",
    "    \n",
    "    return training_set, validation_set, testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set, testing_set = get_sets(training_subjects, validation_subjects, testing_subjects,\n",
    "                                                    train_transform=train_transform, val_transform=train_transform)\n",
    "\n",
    "print('Training set:', len(training_set), 'subjects')\n",
    "print('Validation set:', len(validation_set), 'subjects')\n",
    "print('Testing set:', len(testing_set), 'subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS_DIMENSION = 6\n",
    "SPATIAL_DIMENSIONS = 2, 3, 4\n",
    "\n",
    "VENTRCL =  [4,5,15,43,44,72]# 1\n",
    "BRN_STEM = [16] # 2\n",
    "HIPPOCMPS = [17, 53] # 3\n",
    "AMYGDL = [18, 54] # 4\n",
    "GM = [1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013,\n",
    "       1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024,\n",
    "       1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035,\n",
    "       2000, 2001, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011,\n",
    "       2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022,\n",
    "       2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033,\n",
    "       2034, 2035] # 5\n",
    "\n",
    "LABELS = VENTRCL + BRN_STEM + HIPPOCMPS + AMYGDL + GM # all of interest\n",
    "\n",
    "\n",
    "def prepare_aseg(targets):\n",
    "    \"\"\"\n",
    "    The function binarises the data  with the LABEL list.\n",
    "   \"\"\"\n",
    "    targets = np.where(np.isin(targets, LABELS, invert = True), 0, targets)\n",
    "    targets = np.where(np.isin(targets, VENTRCL), 1, targets)\n",
    "    targets = np.where(np.isin(targets, BRN_STEM), 2, targets)\n",
    "    targets = np.where(np.isin(targets, HIPPOCMPS), 3, targets)\n",
    "    targets = np.where(np.isin(targets, AMYGDL), 4, targets)\n",
    "    targets = np.where(np.isin(targets, GM), 5, targets)\n",
    "\n",
    "\n",
    "    return targets\n",
    "\n",
    "def prepare_batch(batch, device):\n",
    "    \"\"\"\n",
    "    The function loaging *nii.gz files, sending to the devise.\n",
    "    For the LABEL in binarises the data.\n",
    "    \"\"\"\n",
    "    inputs = batch[MRI][DATA].to(device)\n",
    "    targets = batch[LABEL][DATA]\n",
    "    targets = torch.from_numpy(prepare_aseg(targets))\n",
    "    targets = targets.to(device)    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_central_cuts(validation_set[1][MRI][DATA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_aseg(validation_set[1][LABEL][DATA][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(prepare_aseg(seg[DATA])[0]))\n",
    "\n",
    "plot_central_cuts(prepare_aseg(validation_set[1][LABEL][DATA][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mSJoCf0PzA4V"
   },
   "source": [
    "The data is really heavy, so lets try to start with 1 subject/ batch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "olI1JmOGvotO"
   },
   "source": [
    "#### Defining the model and optimizer for training\n",
    "\n",
    "At first check if we have GPU onborad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-I-NoHhMwHMl",
    "outputId": "77afbe44-2ebd-4c41-95ca-88822f1bceae"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46QNsxiDxpcy"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\" Multiclass loss\"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def _one_hot_encoder(self, input_tensor):\n",
    "        tensor_list = []\n",
    "        for i in range(self.n_classes):\n",
    "            temp_prob = input_tensor == i  # * torch.ones_like(input_tensor)\n",
    "            tensor_list.append(temp_prob.unsqueeze(1))\n",
    "        output_tensor = torch.cat(tensor_list, dim=1)\n",
    "        return output_tensor.float()\n",
    "\n",
    "    def _dice_loss(self, score, target):\n",
    "        target = target.float()\n",
    "        smooth = 1e-5\n",
    "        intersect = torch.sum(score * target)\n",
    "        y_sum = torch.sum(target * target)\n",
    "        z_sum = torch.sum(score * score)\n",
    "        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "        loss = 1 - loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, target, weight=None, softmax=False):\n",
    "        if softmax:\n",
    "            inputs = torch.softmax(inputs, dim=1)\n",
    "        target = self._one_hot_encoder(target)\n",
    "        if weight is None:\n",
    "            weight = [1] * self.n_classes\n",
    "        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n",
    "        class_wise_dice = []\n",
    "        loss = 0.0\n",
    "        for i in range(0, self.n_classes):\n",
    "            dice = self._dice_loss(inputs[:, i], target[:, i])\n",
    "            class_wise_dice.append(1.0 - dice.item())\n",
    "            loss += dice * weight[i]\n",
    "        return loss / self.n_classes, class_wise_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CTdHHAt0aYW"
   },
   "outputs": [],
   "source": [
    "def get_model_and_optimizer(device, num_encoding_blocks=4, out_channels_first_layer=8, patience=3):\n",
    "    #Better to train with num_encoding_blocks >=3, out_channels_first_layer>=4 '''\n",
    "    #repoducibility\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    # added reproducibility for cuda\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    #\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "    model = UNet(\n",
    "          in_channels=1,\n",
    "          out_classes=6,\n",
    "          dimensions=3,\n",
    "          num_encoding_blocks=num_encoding_blocks,\n",
    "          out_channels_first_layer=out_channels_first_layer,\n",
    "          normalization='batch',\n",
    "          upsampling_type='linear',\n",
    "          padding=True,\n",
    "          activation='PReLU',\n",
    "      ).to(device)\n",
    "      \n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience, threshold=0.01)\n",
    "    \n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "# model, optimizer, scheduler = get_model_and_optimizer(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sOzdJAykItfw"
   },
   "outputs": [],
   "source": [
    "def get_loaders(training_set, validation_set):\n",
    "    patches_training_set = torchio.Queue(\n",
    "        subjects_dataset=training_set,\n",
    "        max_length=max_queue_length,\n",
    "        samples_per_volume=samples_per_volume,\n",
    "        sampler=torchio.sampler.UniformSampler(patch_size),\n",
    "        num_workers=num_training_workers,\n",
    "        shuffle_subjects=True,\n",
    "        shuffle_patches=True,\n",
    "    )\n",
    "\n",
    "    patches_validation_set = torchio.Queue(\n",
    "        subjects_dataset=validation_set,\n",
    "        max_length=max_queue_length,\n",
    "        samples_per_volume=samples_per_volume,\n",
    "        sampler=torchio.sampler.UniformSampler(patch_size),\n",
    "        num_workers=num_validation_workers,\n",
    "        shuffle_subjects=False,\n",
    "        shuffle_patches=False,\n",
    "    )\n",
    "\n",
    "    training_loader = torch.utils.data.DataLoader(\n",
    "        patches_training_set, batch_size=training_batch_size)\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        patches_validation_set, batch_size=validation_batch_size, shuffle=False)\n",
    "    \n",
    "    return training_loader, validation_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEOsawhiItf3"
   },
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "from einops import rearrange\n",
    "from collections import Counter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "summary_freq_train = 50\n",
    "summary_freq_val = 10\n",
    "\n",
    "def evaluate(model, evaluation_set, patch_size=64, patch_overlap=0, epoch=None):\n",
    "    if epoch is None:\n",
    "        epoch = 0\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    dice_loss = DiceLoss(6)\n",
    "    \n",
    "    for i in tqdm(range(len(evaluation_set)), leave=False):\n",
    "        sample = evaluation_set[i]\n",
    "        input_tensor = sample[MRI][DATA][0]\n",
    "        targets = torch.from_numpy(\n",
    "            prepare_aseg(sample[LABEL][DATA])\n",
    "        \n",
    "        )\n",
    "        \n",
    "        grid_sampler = torchio.inference.GridSampler(\n",
    "            sample,\n",
    "            patch_size,\n",
    "            patch_overlap,\n",
    "        )\n",
    "        patch_loader = torch.utils.data.DataLoader(\n",
    "            grid_sampler, batch_size=validation_batch_size, num_workers=num_validation_workers)\n",
    "\n",
    "        patch_loader = torch.utils.data.DataLoader(\n",
    "            grid_sampler, batch_size=validation_batch_size)\n",
    "    \n",
    "        aggregator = torchio.inference.GridAggregator(grid_sampler)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for patches_batch in patch_loader:\n",
    "                inputs = patches_batch[MRI][DATA].to(device)\n",
    "                locations = patches_batch['location']\n",
    "                logits = model(inputs.float())\n",
    "        \n",
    "                aggregator.add_batch(logits, locations)\n",
    "            \n",
    "            prediction = aggregator.get_output_tensor()\n",
    "            dice_loss_, dice_score_ = dice_loss(prediction.unsqueeze(0), \n",
    "                                                targets, \n",
    "                                                softmax=True)\n",
    "            if (i % summary_freq_val == 0):\n",
    "                pred = F.softmax(prediction, dim=0).argmax(0,True)\n",
    "                experiment.log_figure('validation predictions', plot_predicted(input_tensor, pred, targets),\n",
    "                                      step=i + epoch*len(evaluation_set))\n",
    "            dice_scores.append(dice_score_)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'dice': dice_scores\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def train(num_epochs, training_loader, validation_set, model, optimizer, start_epoch=0, scheduler=None,\n",
    "          weights_stem='', patch_size=64, patch_overlap=0):\n",
    "    \n",
    "    scores = evaluate(model, validation_set, patch_size=patch_size, patch_overlap=patch_overlap, epoch=start_epoch)\n",
    "    \n",
    "    for key in scores:\n",
    "        scores[key] = np.mean(scores[key])\n",
    "        experiment.log_metric(f\"avg_val_{key}\", scores[key], step=0, epoch=start_epoch)\n",
    "        \n",
    "    best_dice = scores['dice']\n",
    "    print(f\"Validation mean score: DICE {scores['dice']:0.3f}\", \"by class:\",scores[key] )    \n",
    "    \n",
    "    step_counter = Counter()\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir, f'model_{weights_stem}.pth'))\n",
    "    for epoch_idx in range(start_epoch+1, start_epoch + num_epochs + 1):\n",
    "        print('\\nStarting epoch', epoch_idx)\n",
    "        run_epoch(epoch_idx, Action.TRAIN, training_loader, model, optimizer, step_counter,\n",
    "                  scheduler=scheduler)\n",
    "        \n",
    "        scores = evaluate(model, validation_set, patch_size=patch_size, patch_overlap=patch_overlap, epoch=epoch_idx)\n",
    "        for key in scores:\n",
    "            scores[key] = np.mean(scores[key])\n",
    "            experiment.log_metric(f\"avg_val_{key}\", scores[key], step=epoch_idx, epoch=epoch_idx)\n",
    "            \n",
    "        print(f\"Validation mean score: DICE {scores['dice']:0.3f}\")    \n",
    "        \n",
    "        avg_dice = scores['dice']\n",
    "        if avg_dice > best_dice:\n",
    "            best_dice = avg_dice\n",
    "            torch.save(model.state_dict(), os.path.join(model_dir, f'model_{weights_stem}.pth'))\n",
    "        \n",
    "        \n",
    "def run_epoch(epoch_idx, action, loader, model, optimizer, step_counter, scheduler=None, loss=DiceLoss):\n",
    "    is_training = action == Action.TRAIN\n",
    "    if isinstance(loss, DiceLoss):\n",
    "        dice_loss = loss(6)\n",
    "    elif isinstance(loss, FocalLoss):\n",
    "        dice_loss = loss(alpha=0.99, gamma=0.2)\n",
    "    elif isinstance(loss, TverskyLoss):\n",
    "        dice_loss = loss(alpha=0.3, gamma=0.75)\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "        \n",
    "    epoch_losses = []\n",
    "    model.train(is_training)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(loader, leave=False)):\n",
    "        inputs, targets = prepare_batch(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            logits = model(inputs.float())\n",
    "            batch_losses, score_dice = dice_loss(logits, \n",
    "                                                 targets.squeeze(1), \n",
    "                                                 softmax=True)\n",
    "            batch_loss = batch_losses.mean() \n",
    "            if is_training:\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            epoch_losses.append(batch_loss.item())\n",
    "            if action == Action.TRAIN:\n",
    "                experiment.log_metric(\"train_dice_loss\", batch_loss.item(),\n",
    "                                      epoch=epoch_idx, step=step_counter[action])\n",
    "                # log images with lower frequency\n",
    "                if batch_idx % summary_freq_train == 0:\n",
    "                    probabilities = F.softmax(logits, dim=1)\n",
    "                    batch_size, _, patch_d = inputs.shape[:3]\n",
    "                    images_grid = []\n",
    "                    \n",
    "                    for sample_idx in range(batch_size):\n",
    "                        # appending the predicted and gt picture (central slices)\n",
    "                        pred = 1 - torch.argmax(logits[sample_idx], dim=0)\n",
    "\n",
    "                        pred_slice = pred[patch_d // 2]\n",
    "                        gt_slice   = targets[sample_idx, 0, patch_d // 2]\n",
    "                        images_grid.append(make_grid([rearrange(pred_slice.float(), '(c h) w -> c h w',  c=1), \n",
    "                                                      rearrange(gt_slice.float(), '(c h) w -> c h w', c=1)], \n",
    "                                                     nrow=1))          \n",
    "\n",
    "                    del probabilities\n",
    "                \n",
    "                    grid_img = make_grid(images_grid)\n",
    "                    grid_img = rearrange(grid_img.cpu().numpy(), 'c h w -> h w c')\n",
    "                    experiment.log_image(grid_img, name='train patches (odd rows -- pred, even rows -- gt)',\n",
    "                                         step=step_counter[action])\n",
    "                        \n",
    "            elif action == Action.VALIDATE:\n",
    "                experiment.log_metric(\"val_dice_loss\", batch_loss.item(),\n",
    "                                      epoch=epoch_idx, step=step_counter[action])\n",
    "            step_counter[action] += 1\n",
    "    \n",
    "    epoch_losses = np.array(epoch_losses)\n",
    "    avg_loss = epoch_losses.mean()\n",
    "    \n",
    "    if action == Action.TRAIN:\n",
    "        experiment.log_metric(\"avg_train_dice_loss\", avg_loss, step=epoch_idx, epoch=epoch_idx)\n",
    "        if scheduler:\n",
    "            scheduler.step(avg_loss)\n",
    "    \n",
    "    elif action == Action.VALIDATE:\n",
    "        experiment.log_metric(\"avg_val_dice\", 1 - avg_loss, step=epoch_idx, epoch=epoch_idx)\n",
    "    \n",
    "    print(f'{action.value} mean loss: {avg_loss:0.3f}')\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqlxW7dBItfq"
   },
   "source": [
    "## 3. Patch-based segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQb_H1u4-Mzu"
   },
   "source": [
    "Let's define another experiment within the same workspace in `COMET ML`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-x4kQJm-NRG"
   },
   "outputs": [],
   "source": [
    "experiment.set_name(\"6 classes, 4 encoding blocks, 8 out, Patch based, 64 batch, crop with images\")\n",
    "model_dir = './logs/6cls_4enc_8out_patch_64batch_crop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 64\n",
    "samples_per_volume = 8\n",
    "max_queue_length = 240\n",
    "training_batch_size = 16\n",
    "validation_batch_size = 4\n",
    "num_training_workers = 8\n",
    "num_validation_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader, validation_loader = get_loaders(training_set, validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler = get_model_and_optimizer(device)\n",
    "weights_stem = '6_classes_4_blocks_8_chanels'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "    \n",
    "train(num_epochs, training_loader, validation_set, model, optimizer, scheduler,\n",
    "      weights_stem=weights_stem, patch_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking\n",
    "enum = enumerate(tqdm(training_loader))\n",
    "_, batch = next(enum)\n",
    "inputs, targets = prepare_batch(batch, 'cuda:0')\n",
    "\n",
    "\n",
    "plt.imshow(inputs.cpu()[2, 0, 32])\n",
    "plt.show()\n",
    "plt.imshow(targets.cpu()[2, 0, 32])\n",
    "plt.show()\n",
    "\n",
    "labels = model(inputs.float())\n",
    "plt.imshow(labels.cpu().detach().numpy()[2, 0, 32])\n",
    "plt.show()\n",
    "\n",
    "print(targets.size(), labels.size())\n",
    "dice_loss = DiceLoss(6)\n",
    "\n",
    "dice_loss_, dice = dice_loss(labels, targets.squeeze(1), softmax=True)\n",
    "print(targets.squeeze(1).size(), labels.size())\n",
    "\n",
    "pred = labels.argmax(1,True) #([16, 1, 64, 64, 64])\n",
    "plt.imshow(pred.cpu().numpy()[2,0, 32])\n",
    "plt.show()\n",
    "\n",
    "print(dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(model_dir, f'model_{weights_stem}.pth'), map_location=device))\n",
    "test_scores = evaluate(model, testing_set, patch_size=64, patch_overlap=0, epoch=0)\n",
    "print(test_scores)\n",
    "print(f\"\\nTesting mean score: DICE {np.mean(test_scores['dice']):0.3f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log_metric(\"avg_test_dice\", np.mean(test_scores['dice']))\n",
    "for i, subject in enumerate(test_subjects):\n",
    "    experiment.log_metric(f\"test_subj_{subject}_dice\", np.mean(test_scores['dice'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrate prediction for random sample from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gXH2vflNItf6"
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "sample = random.choice(validation_set)\n",
    "input_tensor = sample[MRI][DATA][0]\n",
    "patch_size = 64, 64, 64  # we can user larger or smaller patches for inference\n",
    "patch_overlap = 20\n",
    "grid_sampler = torchio.inference.GridSampler(\n",
    "    sample,\n",
    "    patch_size,\n",
    "    patch_overlap,\n",
    ")\n",
    "patch_loader = torch.utils.data.DataLoader(\n",
    "    grid_sampler, batch_size=validation_batch_size)\n",
    "aggregator = torchio.inference.GridAggregator(grid_sampler)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for patches_batch in patch_loader:\n",
    "        inputs = patches_batch[MRI][DATA].to(device)\n",
    "#         print(inputs.unique())\n",
    "        locations = patches_batch['location']\n",
    "        logits = model(inputs.float())\n",
    "        labels = logits.argmax(dim=1, keepdim=True)\n",
    "        aggregator.add_batch(labels, locations)\n",
    "        \n",
    "plot_central_cuts(aggregator.get_output_tensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MY ADDITION: STARTS HERE ###\n",
    "my_transforms = [transforms.RandomGamma(log_gamma=(-0.3, 0.3)),\n",
    "                 # TODO: check augmentations examples\n",
    "                 transforms.RandomBiasField(),\n",
    "                 transforms.RandomMotion(),\n",
    "                 transforms.RandomGhosting(),\n",
    "                 transforms.OneOf({transforms.RandomNoise(): 0.5, \n",
    "                                   transforms.RandomBlur(): 0.5})\n",
    "                ] # AUGMENTATIONS FOR TRAINING\n",
    "\n",
    "### MY ADDITION: ENDS HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_central_cuts(training_set[0][MRI][DATA], title='No transforms')\n",
    "for t in my_transforms:\n",
    "    plot_central_cuts(t(training_set[0][MRI][DATA]), title=str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Crop(crop),\n",
    "    transforms.Pad(4),\n",
    "    *my_transforms\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    # several_transforms,\n",
    "    transforms.Crop(crop),\n",
    "    transforms.Pad(4)\n",
    "])\n",
    "\n",
    "training_set_augs, validation_set_augs, testing_set_augs = get_sets(training_subjects, validation_subjects, testing_subjects,\n",
    "                                                                    train_transform=train_transform, val_transform=val_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 64\n",
    "samples_per_volume = 8\n",
    "max_queue_length = 240\n",
    "training_batch_size = 16\n",
    "validation_batch_size = 4\n",
    "num_training_workers = 8\n",
    "num_validation_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader, validation_loader = get_loaders(training_set_augs, validation_set_augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_name(\"augmentations + 6 classes, 4 encoding blocks, 8 out, Patch based, 64 batch, crop with images\")\n",
    "model_dir = './logs/augmentations_6cls_4enc_8out_patch_64batch_crop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler = get_model_and_optimizer(device)\n",
    "weights_stem = 'aug_6_classes_4_blocks_8_chanels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "    \n",
    "train(num_epochs, training_loader, validation_set, model, optimizer, start_epoch=0, scheduler=scheduler,\n",
    "      weights_stem=weights_stem, patch_overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MY ADDITION: STARTS HERE ###\n",
    "my_transforms = [torchio.transforms.RandomFlip(axes=(0, 1, 2), flip_probability=0.5),\n",
    "                 torchio.transforms.RandomAffine(),\n",
    "                 transforms.RandomGamma(log_gamma=(-0.3, 0.3)),\n",
    "                 # TODO: check augmentations examples\n",
    "                 transforms.RandomBiasField(),\n",
    "                 transforms.RandomMotion(),\n",
    "                 transforms.RandomGhosting(),\n",
    "                 transforms.RandomNoise(mean=0, std=(0, 3)),\n",
    "                 transforms.RandomBlur(std=(0, 1))\n",
    "                ] # AUGMENTATIONS FOR TRAINING\n",
    "\n",
    "### MY ADDITION: ENDS HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_central_cuts(training_set[0][MRI][DATA], title='No transforms')\n",
    "for t in my_transforms:\n",
    "    plot_central_cuts(t(training_set[0][MRI][DATA]), title=str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Crop(crop),\n",
    "    transforms.Pad(4),\n",
    "    *my_transforms\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    # several_transforms,\n",
    "    transforms.Crop(crop),\n",
    "    transforms.Pad(4)\n",
    "])\n",
    "\n",
    "training_set_augs, validation_set_augs, testing_set_augs = get_sets(training_subjects, validation_subjects, testing_subjects,\n",
    "                                                                    train_transform=train_transform, val_transform=val_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 64\n",
    "samples_per_volume = 8\n",
    "max_queue_length = 240\n",
    "training_batch_size = 16\n",
    "validation_batch_size = 4\n",
    "num_training_workers = 8\n",
    "num_validation_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader, validation_loader = get_loaders(training_set_augs, validation_set_augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_name(\"second augmentations + 6 classes, 4 encoding blocks, 32 out, Patch based, 64 batch, crop with images\")\n",
    "model_dir = './logs/second_augmentations_6cls_4enc_8out_patch_64batch_crop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler = get_model_and_optimizer(device, num_encoding_blocks=4, out_channels_first_layer=32)\n",
    "weights_stem = 'second_aug_6_classes_4_blocks_32_chanels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "    \n",
    "train(num_epochs, training_loader, validation_set, model, optimizer, start_epoch=0, scheduler=scheduler,\n",
    "      weights_stem=weights_stem, patch_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(inputs, target, alpha, gamma):\n",
    "    if len(target.shape) == 5:\n",
    "        tg = target[0,0]\n",
    "    elif len(target.shape) == 4:\n",
    "        tg == target[0]\n",
    "    else:\n",
    "        tg = target\n",
    "        \n",
    "    y_true = F.one_hot(tg.to(torch.int64), CHANNELS_DIMENSION).permute(3, 0, 1, 2) # 6xHxWxD\n",
    "    y_pred = torch.clamp(F.softmax(inputs, dim=0), self.epsilon, 1. - self.epsilon) # 6xHxWxD\n",
    "    pt = (y_true * y_pred).sum(dim=0)\n",
    "\n",
    "    loss = -1 * alpha * torch.pow((1 - pt), gamma) * torch.log(pt)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=1, epsilon=1e-5):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, inputs, target):      \n",
    "        if self.alpha is None:\n",
    "            alpha = torch.ones(CHANNELS_DIMENSION, 1)\n",
    "        if isinstance(self.alpha, float):\n",
    "            alpha = torch.ones(CHANNELS_DIMENSION, 1)*self.alpha\n",
    "        else:\n",
    "            alpha = self.alpha\n",
    "        alpha = torch.squeeze(alpha[target.to(torch.int64)[0]])\n",
    "        \n",
    "        loss = focal_loss(inputs, target, alpha, self.gamma)\n",
    "        \n",
    "        return loss.mean(), None # classwise is not computed\n",
    "    \n",
    "class TverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, gamma=1):\n",
    "        super(AssymetricLoss, self).__init__()\n",
    "        self.assymetric = assymetric\n",
    "        self.weight = weight\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _one_hot_encoder(self, input_tensor):\n",
    "        tensor_list = []\n",
    "        for i in range(self.n_classes):\n",
    "            temp_prob = input_tensor == i  # * torch.ones_like(input_tensor)\n",
    "            tensor_list.append(temp_prob.unsqueeze(1))\n",
    "        output_tensor = torch.cat(tensor_list, dim=1)\n",
    "        return output_tensor.float()\n",
    "\n",
    "    def _tversky_loss(self, score, target):\n",
    "        target = target.float()\n",
    "        smooth = 1e-5\n",
    "        intersect = torch.sum(score * target) # TP\n",
    "        # dice:\n",
    "        # y_sum = torch.sum(target * target) # TP + FN\n",
    "        # z_sum = torch.sum(score * score) # TP + FP\n",
    "        # loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "        # tversky: 2TP / (2TP + alpha*FP + gamma*FN)\n",
    "        fp = torch.sum(score*(1 - target))\n",
    "        fn = torch.sum((1 - score)*target)\n",
    "        \n",
    "        loss = (2*intersect + smooth) / (2*intersect + self.alpha*fp + (1-self.alpha)*fn + smooth)\n",
    "        \n",
    "        loss = torch.pow((1 - loss), self.gamma)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, target, weight=None, softmax=False):\n",
    "        if softmax:\n",
    "            inputs = torch.softmax(inputs, dim=1)\n",
    "        target = self._one_hot_encoder(target)\n",
    "        if weight is None:\n",
    "            weight = [1] * self.n_classes\n",
    "        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n",
    "        class_wise_tv = []\n",
    "        tv_loss = 0.0\n",
    "        for i in range(0, self.n_classes):\n",
    "            tv = self._tversky_loss(inputs[:, i], target[:, i])\n",
    "            class_wise_tv.append(1 - tv.item())\n",
    "            tv_loss += tv * weight[i]\n",
    "        \n",
    "        return tv_loss / self.n_classes, class_wise_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = FocalLoss(alpha=torch.Tensor(range(2, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler = get_model_and_optimizer('cpu', num_encoding_blocks=4, out_channels_first_layer=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./logs/second_augmentations_6cls_4enc_8out_patch_64batch_crop/model_second_aug_6_classes_4_blocks_32_chanels.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "sample = random.choice(validation_set)\n",
    "input_tensor = sample[MRI][DATA][0]\n",
    "targets = torch.from_numpy(prepare_aseg(sample[LABEL][DATA]))\n",
    "patch_size = 64, 64, 64  # we can user larger or smaller patches for inference\n",
    "patch_overlap = 20\n",
    "grid_sampler = torchio.inference.GridSampler(\n",
    "    sample,\n",
    "    patch_size,\n",
    "    patch_overlap,\n",
    ")\n",
    "patch_loader = torch.utils.data.DataLoader(\n",
    "    grid_sampler, batch_size=16)\n",
    "aggregator = torchio.inference.GridAggregator(grid_sampler)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for patches_batch in tqdm(patch_loader):\n",
    "        inputs = patches_batch[MRI][DATA].to('cpu')\n",
    "#         print(inputs.unique())\n",
    "        locations = patches_batch['location']\n",
    "        logits = model(inputs.float())\n",
    "#         labels = logits.argmax(dim=1, keepdim=True)\n",
    "        aggregator.add_batch(logits, locations)\n",
    "        \n",
    "output = aggregator.get_output_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = targets.shape[1]//4\n",
    "plt.imshow(targets[0][i, ...])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output.argmax(dim=0)[i, ...])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(output, targets.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.set_name(\"focal loss + 6 classes, 4 encoding blocks, 32 out, Patch based, 64 batch, crop with images\")\n",
    "model_dir = './logs/focal loss_6cls_4enc_8out_patch_64batch_crop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you get a full solution it your hands, yet keep in mind - it is just your baseline. You should experiment with the augmentations, losses and build a story of your model development."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "mri_3DUnet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
